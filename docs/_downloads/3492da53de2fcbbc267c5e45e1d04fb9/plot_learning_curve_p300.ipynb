{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Within Session P300 with learning curve\n\nThis Example shows how to perform a within session analysis while also\ncreating learning curves for a P300 dataset.\nAdditionally, we will evaluate external code. Make sure to have tdlda installed, which\ncan be found in requirements_external.txt\n\nWe will compare two pipelines :\n\n- Riemannian Geometry with Linear Discriminant Analysis\n- XDAWN and Linear Discriminant Analysis\n\nWe will use the P300 paradigm, which uses the AUC as metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Jan Sosulski\n#\n# License: BSD (3-clause)\n\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom pyriemann.estimation import XdawnCovariances\nfrom pyriemann.spatialfilters import Xdawn\nfrom pyriemann.tangentspace import TangentSpace\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.pipeline import make_pipeline\n\nimport moabb\nfrom moabb.datasets import BNCI2014009\nfrom moabb.evaluations import WithinSessionEvaluation\nfrom moabb.paradigms import P300\n\n\n# getting rid of the warnings about the future (on s'en fout !)\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=RuntimeWarning)\n\n\nmoabb.set_log_level(\"info\")\n\n# This is an auxiliary transformer that allows one to vectorize data\n# structures in a pipeline For instance, in the case of a X with dimensions\n# Nt x Nc x Ns, one might be interested in a new data structure with\n# dimensions Nt x (Nc.Ns)\n\n\nclass Vectorizer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y):\n        \"\"\"fit.\"\"\"\n        return self\n\n    def transform(self, X):\n        \"\"\"transform. \"\"\"\n        return np.reshape(X, (X.shape[0], -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create pipelines\n\nPipelines must be a dict of sklearn pipeline transformer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "processing_sampling_rate = 128\npipelines = {}\n\n# we have to do this because the classes are called 'Target' and 'NonTarget'\n# but the evaluation function uses a LabelEncoder, transforming them\n# to 0 and 1\nlabels_dict = {\"Target\": 1, \"NonTarget\": 0}\n\n# Riemannian geometry based classification\npipelines[\"RG+LDA\"] = make_pipeline(\n    XdawnCovariances(nfilter=5, estimator=\"lwf\", xdawn_estimator=\"scm\"),\n    TangentSpace(),\n    LDA(solver=\"lsqr\", shrinkage=\"auto\"),\n)\n\npipelines[\"Xdw+LDA\"] = make_pipeline(\n    Xdawn(nfilter=2, estimator=\"scm\"), Vectorizer(), LDA(solver=\"lsqr\", shrinkage=\"auto\")\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n\nWe define the paradigm (P300) and use all three datasets available for it.\nThe evaluation will return a dataframe containing AUCs for each permutation\nand dataset size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "paradigm = P300(resample=processing_sampling_rate)\ndataset = BNCI2014009()\n# Remove the slicing of the subject list to evaluate multiple subjects\ndataset.subject_list = dataset.subject_list[1:2]\ndatasets = [dataset]\noverwrite = True  # set to True if we want to overwrite cached results\ndata_size = dict(policy=\"ratio\", value=np.geomspace(0.02, 1, 4))\n# When the training data is sparse, peform more permutations than when we have a lot of data\nn_perms = np.floor(np.geomspace(20, 2, len(data_size[\"value\"]))).astype(int)\n# Guarantee reproducibility\nnp.random.seed(7536298)\nevaluation = WithinSessionEvaluation(\n    paradigm=paradigm,\n    datasets=datasets,\n    data_size=data_size,\n    n_perms=n_perms,\n    suffix=\"examples_lr\",\n    overwrite=overwrite,\n)\n\n\nresults = evaluation.process(pipelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Results\n\nHere we plot the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=[8, 4])\n\nn_subs = len(dataset.subject_list)\n\nif n_subs > 1:\n    r = results.groupby([\"pipeline\", \"subject\", \"data_size\"]).mean().reset_index()\nelse:\n    r = results\n\nsns.pointplot(data=r, x=\"data_size\", y=\"score\", hue=\"pipeline\", ax=ax, palette=\"Set1\")\n\nerrbar_meaning = \"subjects\" if n_subs > 1 else \"permutations\"\ntitle_str = f\"Errorbar shows Mean-CI across {errbar_meaning}\"\nax.set_xlabel(\"Amount of training samples\")\nax.set_ylabel(\"ROC AUC\")\nax.set_title(title_str)\nfig.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}