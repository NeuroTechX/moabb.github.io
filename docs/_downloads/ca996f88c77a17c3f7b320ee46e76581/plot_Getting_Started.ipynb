{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started\n\n\nThis tutorial takes you through a basic working example of how to use this\ncodebase, including all the different components, up to the results\ngeneration. If you'd like to know about the statistics and plotting, see the\nnext tutorial.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Vinay Jayaram <vinayjayaram13@gmail.com>\n#\n# License: BSD (3-clause)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introduction\n--------------------\nTo use the codebase you need an evaluation and a paradigm, some algorithms,\nand a list of datasets to run it all on. You can find those in the following\nsubmodules; detailed tutorials are given for each of them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from moabb.datasets import BNCI2014001\nfrom moabb.paradigms import LeftRightImagery\nfrom moabb.evaluations import CrossSessionEvaluation\nfrom moabb.datasets import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to create pipelines within a script, you will likely need at least\nthe make_pipeline function. They can also be specified via a .yml file. Here\nwe will make a couple pipelines just for convenience\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom moabb.pipelines.features import LogVariance\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you would like to specify the logging level when it is running, you can\nuse the standard python logging commands through the top-level moabb module\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import moabb\nmoabb.set_log_level('info')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create pipelines\n----------------\n\nWe create two pipelines: channel-wise log variance followed by LDA, and\nchannel-wise log variance followed by a cross-validated SVM (note that a\ncross-validation via scikit-learn cannot be described in a .yml file). For\nlater in the process, the pipelines need to be in a dictionary where the key\nis the name of the pipeline and the value is the Pipeline object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipelines = {}\npipelines['AM + LDA'] = make_pipeline(LogVariance(),\n                                      LDA())\nparameters = {'C': np.logspace(-2, 2, 10)}\nclf = GridSearchCV(SVC(kernel='linear'), parameters)\npipe = make_pipeline(LogVariance(), clf)\n\npipelines['AM + SVM'] = pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Datasets\n-----------------\n\nDatasets can be specified in many ways: Each paradigm has a property\n'datasets' which returns the datasets that are appropriate for that paradigm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(LeftRightImagery().datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or you can run a search through the available datasets:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(utils.dataset_search(paradigm='imagery', total_classes=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or you can simply make your own list (which we do here due to computational\nconstraints)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "datasets = [BNCI2014001()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paradigm\n--------------------\n\nParadigms define the events, epoch time, bandpass, and other preprocessing\nparameters. They have defaults that you can read in the documentation, or you\ncan simply set them as we do here. A single paradigm defines a method for\ngoing from continuous data to trial data of a fixed size. To learn more look\nat the tutorial Exploring Paradigms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fmin = 8\nfmax = 35\nparadigm = LeftRightImagery(fmin=fmin, fmax=fmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation\n--------------------\n\nAn evaluation defines how the training and test sets are chosen. This could\nbe cross-validated within a single recording, or across days, or sessions, or\nsubjects. This also is the correct place to specify multiple threads.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluation = CrossSessionEvaluation(paradigm=paradigm, datasets=datasets,\n                                    suffix='examples', overwrite=False)\nresults = evaluation.process(pipelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results are returned as a pandas DataFrame, and from here you can do as you\nwant with them\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(results.head())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}