{
  "cells": [
    {
      "id": "dcdaa29b",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "%pip install moabb",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmarking with MOABB showing the CO2 footprint\n\nThis example shows how to use MOABB to track the CO2 footprint\nusing [CodeCarbon library](https://codecarbon.io/)_.\nFor this example, we will use only one\ndataset to keep the computation time low, but this benchmark is designed\nto easily scale to many datasets. Due to limitation of online documentation\ngeneration, the results are computed on a local cluster but could be easily\nreplicated on your infrastructure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Igor Carrara <igor.carrara@inria.fr>\n#          Bruno Aristimunha <b.aristimunha@gmail.com>\n#          Ethan Davis <davise5@uw.edu>\n#\n# License: BSD (3-clause)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.spatial import ConvexHull\n\nfrom moabb import benchmark, set_log_level\nfrom moabb.analysis.plotting import codecarbon_plot, emissions_summary\nfrom moabb.datasets import BNCI2014_001, Zhou2016\nfrom moabb.paradigms import LeftRightImagery\n\n\nset_log_level(\"info\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the pipelines\n\nTo run this example we use several pipelines, ML and DL (Keras) and also\npipelines that need an optimization of the hyperparameter.\nAll this different pipelines are stored in ``pipelines_codecarbon``\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting the datasets (optional)\n\nIf you want to limit your benchmark on a subset of datasets, you can use the\n``include_datasets`` and ``exclude_datasets`` arguments. You will need either\nto provide the dataset's object, or a dataset's code. To get the list of\navailable dataset's code for a given paradigm, you can use the following\ncommand:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "paradigm = LeftRightImagery()\nfor d in paradigm.datasets:\n    print(d.code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we will use only the last dataset, 'Zhou 2016', considering\nonly the first subject.\n\n## Running the benchmark\n\nThe benchmark is run using the ``benchmark`` function. You need to specify the\nfolder containing the pipelines to use, the kind of evaluation and the paradigm\nto use. By default, the benchmark will use all available datasets for all\nparadigms listed in the pipelines. You could restrict to specific evaluation\nand paradigm using the ``evaluations`` and ``paradigms`` arguments.\n\nTo save computation time, the results are cached. If you want to re-run the\nbenchmark, you can set the ``overwrite`` argument to ``True``.\n\nIt is possible to indicate the folder to cache the results and the one to\nsave the analysis & figures. By default, the results are saved in the\n``results`` folder, and the analysis & figures are saved in the ``benchmark``\nfolder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = Zhou2016()\ndataset2 = BNCI2014_001()\ndataset.subject_list = dataset.subject_list[:1]\ndataset2.subject_list = dataset2.subject_list[:1]\ndatasets = [dataset, dataset2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuring CodeCarbon Tracking\n\nThe ``benchmark`` function supports CodeCarbon configuration through the\n``codecarbon_config`` parameter. This allows fine-grained control over how\nemissions are tracked and reported.\n\nCodeCarbon provides many configuration options:\n\n**Output Options:**\n - ``save_to_file`` (bool): Save results to CSV file (default: False)\n - ``log_level`` (str): Logging verbosity (default: 'error')\n - ``output_dir`` (str): Directory for output files (default: '.')\n - ``output_file`` (str): CSV filename (default: 'emissions.csv')\n\n**Tracking Options:**\n - ``tracking_mode`` (str): 'machine' for system-wide, 'process' for isolated\n - ``measure_power_secs`` (int): Power measurement interval in seconds\n - ``experiment_name`` (str): Label for the experiment\n - ``project_name`` (str): Project identifier\n\n**Hardware Options:**\n - ``gpu_ids`` (str): Comma-separated GPU IDs to track\n - ``force_cpu_power`` (float): Manual CPU power in watts\n - ``force_ram_power`` (float): Manual RAM power in watts\n\n**API & Output Backends:**\n - ``save_to_api`` (bool): Send data to CodeCarbon API\n - ``api_endpoint`` (str): Custom API endpoint\n - ``save_to_prometheus`` (bool): Push to Prometheus\n - ``prometheus_url`` (str): Prometheus server address\n\n**Location & Electricity:**\n - ``country_2letter_iso_code`` (str): Country code for carbon intensity\n - ``electricitymaps_api_token`` (str): API token for real-time data\n - ``pue`` (float): Power Usage Effectiveness of data center\n\nExample 1: Basic configuration with CSV output and verbose logging\nNote: Using 'process' tracking mode requires fewer system permissions than 'machine' mode\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "codecarbon_config = {\n    \"save_to_file\": True,\n    \"log_level\": \"error\",\n    \"output_file\": \"emissions_results.csv\",\n    \"experiment_name\": \"MOABB_Benchmark_Zhou2016\",\n    \"tracking_mode\": \"process\",  # Use process-level tracking to reduce permission requirements\n}\n\nresults = benchmark(\n    pipelines=\"./pipelines_codecarbon/\",\n    evaluations=[\"WithinSession\"],\n    paradigms=[\"LeftRightImagery\"],\n    include_datasets=datasets,\n    results=\"./results/\",\n    overwrite=False,\n    plot=False,\n    output=\"./benchmark/\",\n    codecarbon_config=codecarbon_config,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmark prints a summary of the results. Detailed results are saved in a\npandas dataframe, and can be used to generate figures. The analysis & figures\nare saved in the ``benchmark`` folder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results.head()\n\norder_list = [\n    \"CSP + SVM\",\n    \"Tangent Space LR\",\n    \"EN Grid\",\n    \"CSP + LDA Grid\",\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comprehensive CodeCarbon Visualization Analysis\n\nThe ``codecarbon_plot`` function provides multiple visualization modes to\nanalyze emissions data from different perspectives. Each mode answers specific\nquestions about the sustainability and efficiency of your pipelines.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization Mode 1: Basic CO2 Emissions (Default)\n\nThis shows the raw CO2 emissions per dataset and algorithm. It helps you\nunderstand which combinations of dataset and pipeline produce the most\nemissions.\n\n**What it shows:**\n - X-axis: Different datasets used in benchmarking\n - Y-axis: CO2 emissions in kg (log scale)\n - Colors: Different pipeline algorithms\n\n**Best for:** Understanding overall emissions impact\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig1 = codecarbon_plot(results, order_list, country=\"(France)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization Mode 2: Energy Efficiency Analysis\n\nThis mode adds a subplot showing energy efficiency, calculated as:\n**Efficiency = Accuracy Score / CO2 Emissions (kg)**\n\nHigher efficiency means the pipeline achieves better accuracy with less\ncarbon cost. This is the key metric for sustainable machine learning.\n\n**What it shows:**\n - Bar chart: Pipelines ranked by energy efficiency\n - Values: Efficiency score (higher is better)\n - Colors: Pipeline identification\n\n**Best for:** Identifying which pipelines are most sustainable\n**Use case:** When you care about accuracy-to-emissions ratio\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig2 = codecarbon_plot(\n    results,\n    order_list,\n    country=\"(France)\",\n    include_efficiency=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization Mode 3: Complete Analysis with Pareto Frontier\n\nThis comprehensive mode shows ALL three visualizations:\n 1. CO2 emissions per dataset (shows raw environmental impact)\n 2. Energy efficiency ranking (shows best accuracy/emissions ratio)\n 3. Accuracy vs emissions scatter (shows performance-sustainability trade-off)\n\nThe third plot shows the **Pareto frontier**: pipelines in the upper-right\nare Pareto-optimal (you cannot improve accuracy without increasing emissions\nor vice versa).\n\n**What each plot shows:**\n - Plot 1: Raw emissions across datasets and pipelines\n - Plot 2: Which pipelines are most efficient (sorted ranking)\n - Plot 3: Accuracy vs emissions scatter (find the best balance)\n\n**Best for:** Complete sustainability analysis and informed decision-making\n**Use case:** Selecting the best pipeline considering both performance\n             and environmental impact\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig3 = codecarbon_plot(\n    results,\n    order_list,\n    country=\"(France)\",\n    include_efficiency=True,\n    include_power_vs_score=True,\n)\nprint(\"Mode 3 created: Complete analysis with Pareto frontier visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CodeCarbon Configuration Examples\n\nBelow are additional configuration examples for different use cases:\n\n**Example 2: Process-level tracking with custom tracking interval**\n```python\ncodecarbon_config = {\n    'tracking_mode': 'process',\n    'measure_power_secs': 30,\n    'save_to_file': True,\n    'log_level': 'debug'\n}\n```\n**Example 3: GPU tracking with specific IDs**\n```python\ncodecarbon_config = {\n    'gpu_ids': '0,1,2',  # Track GPUs 0, 1, 2\n    'save_to_file': True,\n    'experiment_name': 'multi_gpu_benchmark'\n}\n```\n**Example 4: Real-time carbon intensity data with Electricity Maps API**\n```python\ncodecarbon_config = {\n    'electricitymaps_api_token': 'your-token-here',\n    'country_2letter_iso_code': 'FR',\n    'save_to_file': True,\n    'output_file': 'emissions_real_time.csv'\n}\n```\n**Example 5: API-based tracking and reporting**\n```python\ncodecarbon_config = {\n    'save_to_api': True,\n    'api_endpoint': 'https://api.codecarbon.io',\n    'api_key': 'your-api-key',\n    'project_name': 'MOABB_Project'\n}\n```\n**Example 6: Prometheus metrics export**\n```python\ncodecarbon_config = {\n    'save_to_prometheus': True,\n    'prometheus_url': 'http://localhost:9091',\n    'experiment_name': 'moabb_metrics'\n}\n```\n**Example 7: Custom data center with manual power specifications**\n```python\ncodecarbon_config = {\n    'force_cpu_power': 150.0,  # Watts\n    'force_ram_power': 20.0,   # Watts\n    'pue': 1.2,                # Data center PUE\n    'save_to_file': True\n}\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Emissions Summary Report and Analysis\n\nBeyond visualizations, you can generate a detailed summary report using\nthe ``emissions_summary`` function. This provides comprehensive metrics\nfor data-driven decision making.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "summary = emissions_summary(results, order_list=order_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Summary Visualizations\n\nInstead of text summaries, we create comprehensive visualizations that\nshow the relationships between accuracy, efficiency, and emissions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig_summary, axes = plt.subplots(2, 2, figsize=(14, 10))\nfig_summary.suptitle(\n    \"Emissions Summary: Accuracy, Efficiency, and Environmental Impact\",\n    fontsize=16,\n    fontweight=\"bold\",\n)\n\n# Plot 1: Pipeline Efficiency Rankings\nax1 = axes[0, 0]\nsummary_sorted = summary.sort_values(\"efficiency\", ascending=True)\ncolors = plt.cm.RdYlGn(\n    (summary_sorted[\"efficiency\"] - summary_sorted[\"efficiency\"].min())\n    / (summary_sorted[\"efficiency\"].max() - summary_sorted[\"efficiency\"].min())\n)\nax1.barh(range(len(summary_sorted)), summary_sorted[\"efficiency\"], color=colors)\nax1.set_yticks(range(len(summary_sorted)))\nax1.set_yticklabels(summary_sorted.index)\nax1.set_xlabel(\"Efficiency (Accuracy / kg CO2)\")\nax1.set_title(\"Pipeline Efficiency Ranking\\n(Higher is Better)\")\nax1.grid(axis=\"x\", alpha=0.3)\n\n# Plot 2: Average Emissions Comparison\nax2 = axes[0, 1]\nsummary_sorted_emissions = summary.sort_values(\"avg_emissions\", ascending=False)\ncolors_emissions = plt.cm.Blues(np.linspace(0.4, 1, len(summary_sorted_emissions)))\nax2.bar(\n    range(len(summary_sorted_emissions)),\n    summary_sorted_emissions[\"avg_emissions\"],\n    color=colors_emissions,\n)\nax2.set_xticks(range(len(summary_sorted_emissions)))\nax2.set_xticklabels(summary_sorted_emissions.index, rotation=45, ha=\"right\")\nax2.set_ylabel(\"Average CO2 Emissions (kg/eval)\")\nax2.set_title(\"Carbon Footprint per Pipeline\\n(Lower is Better)\")\nax2.grid(axis=\"y\", alpha=0.3)\n\n# Plot 3: Accuracy Distribution with Standard Deviation\nax3 = axes[1, 0]\nsummary_sorted_score = summary.sort_values(\"avg_score\", ascending=False)\nx_pos = np.arange(len(summary_sorted_score))\nax3.bar(\n    x_pos,\n    summary_sorted_score[\"avg_score\"],\n    yerr=summary_sorted_score[\"std_score\"],\n    capsize=5,\n    color=\"steelblue\",\n    alpha=0.7,\n)\nax3.set_xticks(x_pos)\nax3.set_xticklabels(summary_sorted_score.index, rotation=45, ha=\"right\")\nax3.set_ylabel(\"Average Score\")\nax3.set_title(\"Accuracy Performance with Variability\\n(Higher is Better)\")\nax3.set_ylim([0, 1.0])\nax3.grid(axis=\"y\", alpha=0.3)\n\n# Plot 4: Total Emissions Summary\nax4 = axes[1, 1]\nsummary_sorted_total = summary.sort_values(\"total_emissions\", ascending=False)\ncolors_total = plt.cm.Oranges(np.linspace(0.4, 1, len(summary_sorted_total)))\nax4.bar(\n    range(len(summary_sorted_total)),\n    summary_sorted_total[\"total_emissions\"],\n    color=colors_total,\n)\nax4.set_xticks(range(len(summary_sorted_total)))\nax4.set_xticklabels(summary_sorted_total.index, rotation=45, ha=\"right\")\nax4.set_ylabel(\"Total CO2 Emissions (kg)\")\nax4.set_title(\"Total Carbon Footprint per Pipeline\\n(Lower is Better)\")\nax4.grid(axis=\"y\", alpha=0.3)\n\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pareto Frontier for Decision Making\n\nThe Pareto frontier visualization helps identify the best trade-off\nbetween accuracy and environmental impact. Points on the frontier are\nPareto-optimal: you cannot improve accuracy without increasing emissions\nor vice versa.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig_pareto, ax = plt.subplots(figsize=(10, 8))\n\n# Calculate Pareto frontier\npoints = summary[[\"avg_score\", \"avg_emissions\"]].values\nhull = ConvexHull(points)\nfrontier_indices = hull.vertices\n\n# Plot all pipelines\nfor idx, pipeline in enumerate(summary.index):\n    if idx in frontier_indices:\n        ax.scatter(\n            summary.loc[pipeline, \"avg_emissions\"],\n            summary.loc[pipeline, \"avg_score\"],\n            s=300,\n            alpha=0.8,\n            edgecolors=\"darkgreen\",\n            linewidth=2,\n            label=pipeline if idx < 3 else \"\",\n        )\n    else:\n        ax.scatter(\n            summary.loc[pipeline, \"avg_emissions\"],\n            summary.loc[pipeline, \"avg_score\"],\n            s=200,\n            alpha=0.5,\n            color=\"gray\",\n        )\n    ax.annotate(\n        pipeline,\n        (summary.loc[pipeline, \"avg_emissions\"], summary.loc[pipeline, \"avg_score\"]),\n        xytext=(5, 5),\n        textcoords=\"offset points\",\n        fontsize=9,\n    )\n\n# Highlight Pareto frontier\nfrontier_points = points[frontier_indices]\nfrontier_points = frontier_points[np.argsort(frontier_points[:, 0])]\nax.plot(\n    frontier_points[:, 0],\n    frontier_points[:, 1],\n    \"g--\",\n    linewidth=2,\n    label=\"Pareto Frontier\",\n)\n\nax.set_xlabel(\"Average CO2 Emissions (kg/eval)\", fontsize=12)\nax.set_ylabel(\"Average Accuracy Score\", fontsize=12)\nax.set_title(\n    \"Pareto Frontier: Accuracy vs Emissions Trade-off\", fontsize=14, fontweight=\"bold\"\n)\nax.grid(True, alpha=0.3)\nax.legend(loc=\"best\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result expected will be the following image, but varying depending on the\n machine and the country used to run the example.\n\n .. image:: ../../images/example_codecarbon.png\n    :align: center\n    :alt: carbon_example\n\n##############################################################################\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}